# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: inference.proto

import sys
_b = sys.version_info[0] < 3 and (lambda x: x) or (
    lambda x: x.encode('latin1'))
from google.protobuf import descriptor as _descriptor
from google.protobuf import message as _message
from google.protobuf import reflection as _reflection
from google.protobuf import symbol_database as _symbol_database
from google.protobuf import descriptor_pb2
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()

from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2

DESCRIPTOR = _descriptor.FileDescriptor(
    name='inference.proto',
    package='tensorflow.serving',
    syntax='proto3',
    serialized_pb=_b(
        '\n\x0finference.proto\x12\x12tensorflow.serving\x1a&tensorflow/core/framework/tensor.proto\" \n\x10InferenceRequest\x12\x0c\n\x04\x64\x61ta\x18\x01 \x01(\t\"!\n\x11InferenceResponse\x12\x0c\n\x04\x64\x61ta\x18\x01 \x01(\t2n\n\x10InferenceService\x12Z\n\x0b\x44oInference\x12$.tensorflow.serving.InferenceRequest\x1a%.tensorflow.serving.InferenceResponseb\x06proto3'),
    dependencies=[tensorflow_dot_core_dot_framework_dot_tensor__pb2.DESCRIPTOR,
                  ])
_sym_db.RegisterFileDescriptor(DESCRIPTOR)




_INFERENCEREQUEST = _descriptor.Descriptor(
    name='InferenceRequest',
    full_name='tensorflow.serving.InferenceRequest',
    filename=None,
    file=DESCRIPTOR,
    containing_type=None,
    fields=[
    _descriptor.FieldDescriptor(
      name='data', full_name='tensorflow.serving.InferenceRequest.data', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    ],
    extensions=[
    ],
    nested_types=[],
    enum_types=[
    ],
    options=None,
    is_extendable=False,
    syntax='proto3',
    extension_ranges=[],
    oneofs=[
    ],
    serialized_start=79,
    serialized_end=111,
)


_INFERENCERESPONSE = _descriptor.Descriptor(
    name='InferenceResponse',
    full_name='tensorflow.serving.InferenceResponse',
    filename=None,
    file=DESCRIPTOR,
    containing_type=None,
    fields=[
    _descriptor.FieldDescriptor(
      name='data', full_name='tensorflow.serving.InferenceResponse.data', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    ],
    extensions=[
    ],
    nested_types=[],
    enum_types=[
    ],
    options=None,
    is_extendable=False,
    syntax='proto3',
    extension_ranges=[],
    oneofs=[
    ],
    serialized_start=113,
    serialized_end=146,
)

DESCRIPTOR.message_types_by_name['InferenceRequest'] = _INFERENCEREQUEST
DESCRIPTOR.message_types_by_name['InferenceResponse'] = _INFERENCERESPONSE

InferenceRequest = _reflection.GeneratedProtocolMessageType(
    'InferenceRequest',
    (_message.Message, ),
    dict(
        DESCRIPTOR=_INFERENCEREQUEST,
        __module__='inference_pb2'
        # @@protoc_insertion_point(class_scope:tensorflow.serving.InferenceRequest)
    ))
_sym_db.RegisterMessage(InferenceRequest)

InferenceResponse = _reflection.GeneratedProtocolMessageType(
    'InferenceResponse',
    (_message.Message, ),
    dict(
        DESCRIPTOR=_INFERENCERESPONSE,
        __module__='inference_pb2'
        # @@protoc_insertion_point(class_scope:tensorflow.serving.InferenceResponse)
    ))
_sym_db.RegisterMessage(InferenceResponse)

import grpc
from grpc.beta import implementations as beta_implementations
from grpc.beta import interfaces as beta_interfaces
from grpc.framework.common import cardinality
from grpc.framework.interfaces.face import utilities as face_utilities


class InferenceServiceStub(object):
    def __init__(self, channel):
        """Constructor.

    Args:
      channel: A grpc.Channel.
    """
        self.DoInference = channel.unary_unary(
            '/tensorflow.serving.InferenceService/DoInference',
            request_serializer=InferenceRequest.SerializeToString,
            response_deserializer=InferenceResponse.FromString, )


class InferenceServiceServicer(object):
    def DoInference(self, request, context):
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details('Method not implemented!')
        raise NotImplementedError('Method not implemented!')


def add_InferenceServiceServicer_to_server(servicer, server):
    rpc_method_handlers = {
        'DoInference': grpc.unary_unary_rpc_method_handler(
            servicer.DoInference,
            request_deserializer=InferenceRequest.FromString,
            response_serializer=InferenceResponse.SerializeToString, ),
    }
    generic_handler = grpc.method_handlers_generic_handler(
        'tensorflow.serving.InferenceService', rpc_method_handlers)
    server.add_generic_rpc_handlers((generic_handler, ))


class BetaInferenceServiceServicer(object):
    def DoInference(self, request, context):
        context.code(beta_interfaces.StatusCode.UNIMPLEMENTED)


class BetaInferenceServiceStub(object):
    def DoInference(self,
                    request,
                    timeout,
                    metadata=None,
                    with_call=False,
                    protocol_options=None):
        raise NotImplementedError()

    DoInference.future = None


def beta_create_InferenceService_server(servicer,
                                        pool=None,
                                        pool_size=None,
                                        default_timeout=None,
                                        maximum_timeout=None):
    request_deserializers = {
        ('tensorflow.serving.InferenceService', 'DoInference'):
        InferenceRequest.FromString,
    }
    response_serializers = {
        ('tensorflow.serving.InferenceService', 'DoInference'):
        InferenceResponse.SerializeToString,
    }
    method_implementations = {
        ('tensorflow.serving.InferenceService', 'DoInference'):
        face_utilities.unary_unary_inline(servicer.DoInference),
    }
    server_options = beta_implementations.server_options(
        request_deserializers=request_deserializers,
        response_serializers=response_serializers,
        thread_pool=pool,
        thread_pool_size=pool_size,
        default_timeout=default_timeout,
        maximum_timeout=maximum_timeout)
    return beta_implementations.server(method_implementations,
                                       options=server_options)


def beta_create_InferenceService_stub(channel,
                                      host=None,
                                      metadata_transformer=None,
                                      pool=None,
                                      pool_size=None):
    request_serializers = {
        ('tensorflow.serving.InferenceService', 'DoInference'):
        InferenceRequest.SerializeToString,
    }
    response_deserializers = {
        ('tensorflow.serving.InferenceService', 'DoInference'):
        InferenceResponse.FromString,
    }
    cardinalities = {'DoInference': cardinality.Cardinality.UNARY_UNARY, }
    stub_options = beta_implementations.stub_options(
        host=host,
        metadata_transformer=metadata_transformer,
        request_serializers=request_serializers,
        response_deserializers=response_deserializers,
        thread_pool=pool,
        thread_pool_size=pool_size)
    return beta_implementations.dynamic_stub(
        channel,
        'tensorflow.serving.InferenceService',
        cardinalities,
        options=stub_options)
# @@protoc_insertion_point(module_scope)
